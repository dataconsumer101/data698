---
title: "Failing to Follow the Herd: Factors Explaining Variations in Covid Vaccination Rates in the U.S."
author: "Leo Yi"
date: "`r Sys.Date()`"
output:
  word_document:
    reference_docx: c:/downloads/docs/data698/refs/word_style_reference.docx
    toc: yes
    toc_depth: '4'
bibliography: c:/downloads/docs/data698/refs/references.bib
csl: c:/downloads/docs/data698/refs/apa.csl    
---

```{=html}
<style type="text/css">

  code {
    font-family: "Consolas";
    font-size: 10px;
  }
  
  pre {
    font-family: "Consolas";
    font-size: 10px;
  }
  
  mark {
    background-color: whitesmoke;
    color: black;
    font-size: 10px;
  }
  
  body, td {
   font-size: 10px;
  }
  
  code.r {
    font-size: 10px;
  }

</style>
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE, fig.width = 8)

options(scipen = 9)

library(knitr)

library(ggplot2)
library(scales)
library(tidyr)
library(dplyr)
library(stringr)
library(lubridate)
library(readxl)

library(inspectdf)
library(VIM)
library(vip)
library(corrplot)
library(caret)
library(randomForest)
library(ranger)

library(fpp2)
library(zoo)

library(conflicted)
```

```{r conflicts}
# conflict_scout()
conflict_prefer('filter', 'dplyr')
```

```{r stored files}

# THIS WILL DETERMINE IF THE MARKDOWN FILE IS COMPLETELY REFRESHED OR REFERENCES STORED FILES
full_run <- FALSE


# check current working directory
cwd <- getwd()


# if this is on Leo's local machine use local folder, otherwise get files from git repo
if (cwd == "C:/Downloads/docs/data698") {
  local_source <- TRUE
} else {
  local_source <- FALSE
}

# # local source manual override
# local_source <- FALSE


# path2 is for large files not stored in github
if (local_source) {
  
  path <- 'c:/downloads/docs/data698/data/'  
  path2 <- 'c:/downloads/docs/data698_local/data/'
  project_files_path <- 'c:/downloads/docs/data698/project_files/' 
  
} else {
  
  path <- 'https://raw.githubusercontent.com/dataconsumer101/data698/main/data/'
  project_files_path <- 'https://raw.githubusercontent.com/dataconsumer101/data698/main/project_files/'
  
}

# PROJECT FILES PATH ---
# instead of rerunning models every time the file is knit, I've saved the results locally to be published faster
test_results_path <- paste0(project_files_path, 'pred_test_results.csv')
varimp_path <- paste0(project_files_path, 'top10_varimp.csv')

```

\pagebreak

# Introduction

Over the past two years, humanity has been faced with a common enemy whose name is notoriously known across the world. COVID-19, which is caused by the coronavirus SARS-CoV-2, was first officially identified in January 2020 [@allam]. The response by national leaders across the world has varied from apathy and denial to strict lockdowns. Within the United States, national and state leaders continue to push their own idea of how to face this enemy. Additionally, the public’s polarized opinions about how to handle the situation are also hindering the effectiveness of a unified strategy. These political issues are the reality of a world made up of individuals who work and live together.

The scientific community confronting COVID-19 in the US, led by the CDC, has been offering updated guidance for our government, businesses, and individuals [@cdc]. The guidance was based on the information available at the time and included suggestions of lock downs, face masks, social distancing, quarantines, and vaccines. In December of 2020, the FDA issued Emergency Use Authorization for the Pfizer-BioNTech and Moderna vaccines, followed by the Janssen vaccine in February 2021 [@fda].

Vaccines are proven to be an effective strategy to fight viruses. Each of the vaccines currently available have been proven safe and effective at preventing severe disease [@bajema] and have been suggested and encouraged by the CDC. The federal government set out to vaccinate the public, with the initial strategy focused on logistics – how to produce, supply, deliver, and give the vaccine to every person in the US. As of September 11, 2021 vaccinations are now currently available, for free, for all adults with 53.9% of Americans fully vaccinated [@mayo].

The situation we face today is the lower than ideal vaccination rates across the country. For various personal reasons, people have chosen to refuse the vaccine, with some actively campaigning against it. The issue now relates to social and political differences rather than logistics and the goal of herd immunity seems to be out of reach [@aschwanden].

The purpose of this research is to seek to understand the connections that may exist between people who ignore the science of vaccines and the collective knowledge of humanity to believe only what they choose to believe. If vaccine rates have plateaued and we can assume those who have not yet vaccinated are doubtful of science, this may be the first time we have collected data that can tell us who believes in science and humanity. This research will aim to shed some more light on who we are.

\pagebreak

# Literature Review

As a nation with a goal to reach vaccine levels sufficient for herd immunity, a major obstacle is that some people refuse vaccines. The anti-vaccine movement has existed well before COVID, with unfounded claims that vaccines cause autism. These claims have been spread far enough that the CDC offers their official perspective on it's website in order to clarify the now common misinformation [@autism].

In the modern world, advances in all areas of study make it impossible for any one person to know about everything. An injection of some unknown substance may lead to fear without the previous knowledge and understanding of what a vaccine is and how it works [@hesitation]. Additionally, mistrust of the government, scientists, healthcare workers, or the system altogether can lead to wild speculation about what substances are actually entering our bodies. 

The exposure to competing views of information and misinformation in social and mass media adds to the confusion and hinders people's ability to make a well informed decision [@hussain]. Additionally, people who are led into the anti-vaccination movement may be influenced by the misinformation spread by Donald Trump and the surrounding conservative movement, which provides a social network that is engaging, supportive, and based on feelings of community [@social_media].


\pagebreak

# Research Questions

What factors can explain variations in vaccination rates in the US?

If we create a projection for vaccination rates, when might we hit herd immunity of 70% vaccination rates [@herd_immunity], if ever?

The idea behind these questions comes from the phenomena that people are choosing to go against the best known strategy humanity has formed so far, with no real effective alternative strategy. Before understanding why people might be doing this, the goal in this research is to see if we can find commonalities in people who choose to ignore the herd and the facts and believe their own version of reality. That might lead us to why vaccination rates are not currently high enough to hit herd immunity.

Based on current research, political party affiliation seems to be a major factor affecting vaccination rates. Additionally, it's possible that education would affect people's willingness to accept and trust modern scientific knowledge and also be less likely to be influenced by misinformation campaigns in social media [@social_media]. It's not clear whether other county level factors will show correlations, but the idea is to check whether we can find any statistically signficant factors that affect vaccination rates, in order to further consider why these connections may exist. 

\pagebreak

# Data

The available data for vaccine rates, which will be the independent variable, are collected and aggregated at the country level by the Centers for Disease Control and Prevention [@vaccine_rates]. The Federal Information Processing Standard (FIPS) code will be the level at which all variables will be collected and aggregated. The CDC provides vaccination rates by county and day starting from the end of 2020, excluding Texas.

The US Census provides population data by age, sex, and race [@population], which will be used both as a way to standardize all other variables as a percent of the population, as well as to check if certain age groups or ethnicity might be relevant factors in overall vaccination rates.

The Association of Religion Data Archives provides rates of religious followers by denomination [@religion], however rather than using each minor sect, we'll only be looking at rates of religious people and densities of congregations to see if overall religious beliefs may affect vaccination rates.

The Economic Research Service of the US Department of Agriculture was used to gather socioeconomic data related to unemployment, median household income, education attainment, poverty estimates, a rural-urban scale, and an urban influence factor [@county_socioeconomic]. Additionally, the ERS provides a dataset on economic topology, which provides additional variables related to the level of different industries by county, as well as child poverty levels [@county_topology].

For political variables, the 2020 Election Results by County were used in order to assess the policy perspectives of its constituents [@election]. It should be noted that Alaska is the only state that has different FIPS codes for the counties and voting districts.

An obvious factor that could influence a counties vaccination rates would be the number of cases and deaths [@deaths] experienced. A less obvious set of variables would be natality data, which includes information about babies and their mothers [@births].
  
Finally, some external factors were included, like temperature ranges and average preciptation [@weather], environmental quality [@environmental_quality], and crime statistics [@crime].


```{r import required}

if (local_source) {
  vr_path <- paste0(path2, 'COVID-19_Vaccinations_in_the_United_States_County.csv')
  pop_path <- paste0(path2, 'cc-est2019-alldata.csv')
} else {
  vr_path <- 'https://data.cdc.gov/api/views/8xkx-amqh/rows.csv'
  pop_path <- 'https://www2.census.gov/programs-surveys/popest/datasets/2010-2019/counties/asrh/cc-est2019-alldata.csv'  
}

# Vaccination Rates Raw Data
vr_raw <- read.csv(vr_path) 

# Population Estimates Raw Data
pop_raw <- read.csv(pop_path) 

# 2020 Election Results
election_path <- paste0(path, '2020_US_County_Level_Presidential_Results.csv')
election_raw <- read.csv(election_path)

```

```{r import other, eval = if (full_run) TRUE else FALSE}

# Religious Affliations
rel_path <- paste0(path, 'U.S. Religion Census Religious Congregations and Membership Study, 2010 (County File).XLSX')
rel_raw <- read_excel(rel_path)

# Unemployment, Median HH Income, County Classes
emp_path <- paste0(path, 'Unemployment.xlsx')
emp_raw <- read_excel(emp_path, skip = 4)

# Education
edu_path <- paste0(path, 'Education.xls')
edu_raw <- read_excel(edu_path, skip = 4)  
  
# Poverty Levels
pov_path <- paste0(path, 'PovertyEstimates.xls')
pov_raw <- read_excel(pov_path, skip = 4)

# Economic Typology Classes
et_path <- paste0(path, 'ERSCountyTypology2015Edition.xls')
et_raw <- read_excel(et_path, skip = 3)  

# Covid Cases and Deaths
if (local_source) {
  cases_path <- paste0(path2, 'us-counties.csv')
} else {
  cases_path <- 'https://raw.githubusercontent.com/nytimes/covid-19-data/master/us-counties.csv'
} 

cases_raw <- read.csv(cases_path)

# Birth data by county
births_path <- paste0(path, 'Natality, 2016-2019 expanded.txt')
births_raw <- read.delim(births_path)

# Birth data by state
births2_path <- paste0(path, 'Natality, 2016-2019 expanded_states.txt')
births2_raw <- read.delim(births2_path)

# Annual Precipitation Total
precip_path <- paste0(path, '110-pcp-202108-12.csv')
precip_raw <- read.csv(precip_path, skip = 3)

# Temperature Ranges
temp_min_path <- paste0(path, '110-tmin-202108-12.csv')
temp_max_path <- paste0(path, '110-tmax-202108-12.csv')
temp_avg_path <- paste0(path, '110-tavg-202108-12.csv')
temp_min_raw <- read.csv(temp_min_path, skip = 3)
temp_max_raw <- read.csv(temp_max_path, skip = 3)
temp_avg_raw <- read.csv(temp_avg_path, skip = 3)

# Crime
crime_path <- paste0(path, 'cc_B-08.xls')
crime_raw <- read_excel(crime_path, skip = 7)
  
# Environmental Quality
eq_path <- paste0(path, 'Eqidata_all_domains_2014March11.csv')
eq_raw <- read.csv(eq_path)

```

```{r population adjust}

# Population Rates, Latest Estimates Only
pop <- pop_raw %>%
  filter(YEAR == 12) # 12 = 7/1/2019 population estimate

# lowercase field names
names(pop) <- lapply(names(pop), tolower)

# add FIPS field
pop$fips <- paste0(
  with(pop, ifelse(state < 10, paste0('0', state), state)),
  with(pop, ifelse(county < 10, paste0('00', county), ifelse(county < 100, paste0('0', county), county)))
)

# breakdown of population by age
age_group <- pop %>%
  select(fips, 
         agegrp, 
         tot_pop
         ) %>%
  mutate(agegrp = paste0('ag_', agegrp)) %>%
  spread(agegrp, tot_pop) %>%
  mutate(pct_age_0_4 = ag_1 / ag_0,
         pct_age_5_9 = ag_2 / ag_0,
         pct_age_10_14 = ag_3 / ag_0,
         pct_age_15_19 = ag_4 / ag_0,
         pct_age_20_24 = ag_5 / ag_0,
         pct_age_25_29 = ag_6 / ag_0,
         pct_age_30_34 = ag_7 / ag_0,
         pct_age_35_39 = ag_8 / ag_0,
         pct_age_40_44 = ag_9 / ag_0,
         pct_age_45_49 = ag_10 / ag_0,
         pct_age_50_54 = ag_11 / ag_0,
         pct_age_55_59 = ag_12 / ag_0,
         pct_age_60_64 = ag_13 / ag_0,
         pct_age_65_69 = ag_14 / ag_0,
         pct_age_70_74 = ag_15 / ag_0,
         pct_age_75_79 = ag_16 / ag_0,
         pct_age_80_84 = ag_17 / ag_0,
         pct_age_85_plus = ag_18 / ag_0
         ) 

# keep only fips and percents fields
age_group <- age_group[,c(1, 21:ncol(age_group))]

# demographic population data
dpop <- pop %>%
  filter(agegrp == 0) %>%
  mutate(mf_ratio = tot_male / tot_female,
         pct_white = (wa_male + wa_female) / tot_pop,
         pct_black = (ba_male + ba_female) / tot_pop,
         pct_native = (ia_male + ia_female) / tot_pop,
         pct_asian = (aa_male + aa_female) / tot_pop,
         pct_pacific = (na_male + na_female) / tot_pop,
         pct_biracial = (tom_male + tom_female) / tot_pop,
         pct_wac = (wac_male + wac_female) / tot_pop,
         pct_hispanic = (h_male + h_female) / tot_pop
         ) %>%
  select(fips,
         mf_ratio,
         pct_white,
         pct_black,
         pct_native,
         pct_asian,
         pct_pacific,
         pct_biracial,
         pct_wac,
         pct_hispanic
         )
  

# total population data
tpop <- pop %>%
  filter(agegrp == 0) %>%
  select(fips, tot_pop)

```

```{r religion adjust, eval = if (full_run) TRUE else FALSE}

# overall rates of religion by county
religion <- rel_raw %>%
  as.data.frame()

# lowercase field names
names(religion) <- lapply(names(religion), tolower)

# fips as 5 character string
religion$fips <- as.character(religion$fips)
religion$fips <- ifelse(nchar(religion$fips) == 4, paste0('0', religion$fips), religion$fips)

# calculate metrics and select only relevant fields
religion <- religion %>%
  inner_join(tpop, by = c('fips' = 'fips')) %>%
  mutate(congregations_per_person = totcng / tot_pop,
         congregations_per_adherent = totcng / totadh) %>%
  select(-stcode,
         -stabbr,
         -stname,
         -cntycode,
         -cntyname,
         -pop2010,
         -tot_pop
         )

# force logical fields back to numeric
religion <- religion %>%
  mutate_if(is.logical, as.numeric)

# fill in NA with zero
religion[is.na(religion)] <- 0

religion_summary <- religion %>%
  select(fips,
         totrate,
         congregations_per_person,
         congregations_per_adherent
         )

```


```{r employment adjust, eval = if (full_run) TRUE else FALSE}

# Employment Statistics data frame
emp <- emp_raw %>%
  as.data.frame()
  
# lowercase field names
names(emp) <- lapply(names(emp), tolower)

# select fields from unemployment data set
emp <- emp %>%
  inner_join(tpop, by = c('fips_code' = 'fips')) %>%
  mutate(ue_rate_2019 = unemployment_rate_2019 / 100,
         ue_rate_2020 = unemployment_rate_2020 / 100,
         labor_force_pct_2019 = civilian_labor_force_2019 / tot_pop,
         labor_force_pct_2020 = civilian_labor_force_2020 / tot_pop,
         ruc_code = factor(rural_urban_continuum_code_2013),
         ui_code = factor(urban_influence_code_2013),
         ) %>%
  select(fips = fips_code,
         ruc_code,
         ui_code,
         labor_force_pct_2019,
         labor_force_pct_2020,
         med_hh_income_2019 = median_household_income_2019,
         med_hh_income_state_index = med_hh_income_percent_of_state_total_2019,
         ue_rate_2019,
         ue_rate_2020
         )

```


```{r education adjust, eval = if (full_run) TRUE else FALSE}

# Education Statistics data frame
edu <- edu_raw %>%
  as.data.frame()

# lowercase field names
names(edu) <- lapply(names(edu), tolower)

# str(edu)

edu <- edu %>%
  mutate(sub_hs_pct = `percent of adults with less than a high school diploma, 2015-19` / 100,
         hs_pct = `percent of adults with a high school diploma only, 2015-19` / 100,
         some_college_pct = `percent of adults completing some college or associate's degree, 2015-19` / 100,
         college_pct = `percent of adults with a bachelor's degree or higher, 2015-19` / 100
         ) %>%
  select(fips = `fips code`,
         sub_hs_pct,
         hs_pct,
         some_college_pct,
         college_pct
         )

```

```{r economic typology adjust, eval = if (full_run) TRUE else FALSE}

# Economic Topology Classes
et <- et_raw %>%
  as.data.frame()

# rename fields
names(et) <- c('fips',
               'state',
               'county',
               'metro_status',
               'economic_types',
               'farming',
               'mining',
               'manufacturing',
               'government',
               'recreation',
               'nonspecialized',
               'low_education',
               'low_employment',
               'pop_loss',
               'retirement_destination',
               'persistent_poverty',
               'persistent_child_poverty'
               )

# Exclude state and county fields
et <- et %>%
  select(-state, -county)

# convert fields to factor datatype
et <- et %>%
  mutate_if(is.numeric, as.factor)

```

```{r election adjust}

# 2020 Election Results
election <- election_raw %>%
  as.data.frame() 

# convert fips field to 5 characters
election$fips <- as.character(election$county_fips)
election$fips <- ifelse(nchar(election$fips) == 4, paste0('0', election$fips), election$fips)
election$county_fips <- NULL

# calculate voter turnout and gop/dem split
election <- election %>%
  inner_join(tpop, by = c('fips' = 'fips')) %>%
  mutate(voter_turnout = total_votes / tot_pop) %>%
  select(fips,
         voter_turnout,
         per_gop,
         per_dem)

```

```{r poverty adjust, eval = if (full_run) TRUE else FALSE}

# Poverty Levels
poverty <- pov_raw %>%
  as.data.frame()

# lowercase field names
names(poverty) <- lapply(names(poverty), tolower)

poverty <- poverty %>%
  mutate(pov_pct_all = pctpovall_2019 / 100,
         pov_pct_age_0_17 = pctpov017_2019 / 100,
         pov_pct_age_5_17 = pctpov517_2019 / 100
         ) %>%
  select(fips = fipstxt,
         pov_pct_all,
         pov_pct_age_0_17,
         pov_pct_age_5_17
         )

```





```{r begin data combine}

# Vaccine Rates, base data frame
vr <- vr_raw %>%
  as.data.frame()

# lowercase fields names
names(vr) <- lapply(names(vr), tolower)

# covert data type to date
vr$date <- as.Date(vr$date, format = '%m/%d/%Y')

# convert percentage to decimal format
vr$pct_vaccinated <- vr$series_complete_pop_pct / 100

# join total population to combined data frame
vr <- left_join(vr, tpop, by = c('fips' = 'fips')) 

```

```{r state_plot}

# Vaccine Rates by State and Date
state_rate <- vr %>%
  group_by(date, state = recip_state) %>%
  summarize(vaccinated = sum(series_complete_yes, na.rm = T),
            total_pop = sum(tot_pop, na.rm = T),
            pv = vaccinated / total_pop,
            .groups = 'drop'
            ) %>%
  filter(pv >= 0 & pv <= 1)

# Line chart of vaccination rates over time, by state 2021
state_rate %>%
  filter(date >= '2021-01-01') %>%
  ggplot(aes(x = date, y = pv, color = state)) +
  geom_line() +
  scale_y_continuous(labels = percent_format(1)) +
  scale_x_date(labels = date_format('%b'),
               date_breaks = '1 month') +
  geom_vline(xintercept = as.Date('2021-08-13'), lty = 3) +
  theme_bw() +
  labs(x = '2021', 
       y = 'Vaccination Rate',
       title = 'Vaccination Rates by State')

```

Looking at the plot above showing rates for all states since the beginning of 2021, the vertical dotted line represents August 13th, which is a Friday. This date was chosen as the snapshot date because it is between corrections, before mandates were issued, and also lagged from the present in order to include data that comes in late.

```{r overall_rates}

# Vaccination Rate Nationwide
overall_rate <- vr %>%
  filter(recip_state != 'TX') %>%
  group_by(date) %>%
  summarize(vaccinated = sum(series_complete_yes, na.rm = T),
            total_pop = sum(tot_pop, na.rm = T),
            pv = vaccinated / total_pop,
            .groups = 'drop'
            )

# Line Chart
ggplot(overall_rate, aes(x = date, y = pv)) +
  geom_line() +
  scale_y_continuous(labels = percent_format(1),
                     breaks = seq(0, 0.6, 0.1)) +
  scale_x_date(labels = date_format('%b'),
               date_breaks = '1 month') +
  geom_vline(xintercept = as.Date('2021-08-13'), lty = 3) +
  theme_bw() +
  labs(x = '2021', 
       y = 'Vaccination Rate',
       title = 'US Continental Vaccination Rate')

```



```{r snapshot data}

# filter vaccine rates
vr2 <- vr %>%
  filter(recip_state != 'TX' & recip_state != 'PR' & recip_state != 'GU' & recip_state != 'VI') %>%
  filter(recip_county != 'Unknown County') %>%
  select(fips,
         date,
         state = recip_state,
         county = recip_county,
         vrate = pct_vaccinated,
         vaccinated = series_complete_yes,
         total_population = tot_pop
         )

# Snapshot Date Set
snap_date <- '2021-08-13'

# Vaccination Rates Snapshot, excluding Texas, Territories, and Unknown Counties
vrs <- vr2 %>%
  filter(date == snap_date) %>%
  select(-date,
         -vaccinated)


```

```{r cases adjust, eval = if (full_run) TRUE else FALSE}

# cases and deaths data
cases <- cases_raw %>%
  as.data.frame()

# convert fips field to 5 characters
cases$fips <- as.character(cases$fips)
cases$fips <- ifelse(nchar(cases$fips) == 4, paste0('0', cases$fips), cases$fips)

# cases and deaths by fips at snapshot date
cases <- cases %>%
  filter(date == snap_date) %>%
  inner_join(tpop, by = c('fips' = 'fips')) %>%
  mutate(case_rate = cases / tot_pop,
         death_rate = deaths / tot_pop,
         deaths_per_case = deaths / cases
         ) %>%
  select(fips,
         case_rate,
         death_rate,
         deaths_per_case
         )

# Use NYC case/death rates for Bronx/Queens/New York Counties
nyc_pop <- tpop %>%
  filter(fips == '36005' |   # Bronx
           fips == '36047' | # Brooklyn Kings County
           fips == '36061' | # Manhattan
           fips == '36081' | # Queens
           fips == '36085'   # Staten Island Richmond
           ) %>%
  summarize(nyc_pop = sum(tot_pop))

nyc_case_rates <- cases_raw %>%
  filter(date == snap_date) %>%
  filter(county == 'New York City') %>%
  mutate(case_rate = cases / nyc_pop$nyc_pop,
         death_rate = deaths / nyc_pop$nyc_pop,
         deaths_per_case = deaths / cases,
         key = 1
         ) %>%
  select(case_rate,
         death_rate,
         deaths_per_case,
         key)

nyc_cases <- data.frame(fips = c('36005','36047','36061','36081','36085'), key = 1) %>%
  inner_join(nyc_case_rates, by = c('key' = 'key')) %>%
  select(-key)

cases <- bind_rows(cases, nyc_cases)

```

```{r births adjust, eval = if (full_run) TRUE else FALSE}

# Coalesce County Rates, State Rates, by FIPS
births <- vrs %>%
  select(fips) %>%
  mutate(state_code = as.numeric(substr(fips, 1, 2)),
         county_code = as.numeric(fips)
         ) %>%
  left_join(births_raw, by = c('county_code' = 'County.of.Residence.Code')) %>%
  left_join(births2_raw, by = c('state_code' = 'State.of.Residence.Code')) %>%
  mutate(birth_rate = coalesce(Birth.Rate.x, Birth.Rate.y) / 100,
         fertility_rate = coalesce(Fertility.Rate.x, Fertility.Rate.y) / 100,
         avg_age_of_mother = coalesce(Average.Age.of.Mother..years..x, Average.Age.of.Mother..years..y),
         avg_gest_age = coalesce(Average.OE.Gestational.Age..weeks..x, Average.OE.Gestational.Age..weeks..y),
         avg_birth_weight_grams = coalesce(Average.Birth.Weight..grams..x, Average.Birth.Weight..grams..y),
         avg_pre_pregnancy_bmi = coalesce(Average.Pre.pregnancy.BMI.x, Average.Pre.pregnancy.BMI.y),
         avg_prenatal_visits = coalesce(Average.Number.of.Prenatal.Visits.x, Average.Number.of.Prenatal.Visits.y)
         ) %>%
  select(fips,
         birth_rate,
         fertility_rate,
         avg_age_of_mother,
         avg_gest_age,
         avg_birth_weight_grams,
         avg_pre_pregnancy_bmi,
         avg_prenatal_visits
         )

```


```{r precipitation and temperature adjust, eval = if (full_run) TRUE else FALSE}

# Precipitation Data Adjust
precip <- precip_raw %>%
  as.data.frame()

names(precip) <- lapply(names(precip), tolower)

precip <- precip %>%
  select(location_id = location.id,
         precipitation = value,
         precipitation_avg = x1901.2000.mean
         )

# Temp Min Adjust
temp_min <- temp_min_raw %>%
  as.data.frame()

names(temp_min) <- lapply(names(temp_min), tolower)

temp_min <- temp_min %>%
  select(location_id = location.id,
         temp_min = value,
         temp_min_avg = x1901.2000.mean
         )

# Temp Max Adjust
temp_max <- temp_max_raw %>%
  as.data.frame()

names(temp_max) <- lapply(names(temp_max), tolower)

temp_max <- temp_max %>%
  select(location_id = location.id,
         temp_max = value,
         temp_max_avg = x1901.2000.mean
         )

# Temp Avg Adjust
temp_avg <- temp_avg_raw %>%
  as.data.frame()

names(temp_avg) <- lapply(names(temp_avg), tolower)

temp_avg <- temp_avg %>%
  select(location_id = location.id,
         temp_avg = value,
         temp_avg_avg = x1901.2000.mean
         )

# combine weather data into one data frame
weather_all <- precip %>%
  left_join(temp_min, by = c('location_id' = 'location_id')) %>%
  left_join(temp_max, by = c('location_id' = 'location_id')) %>%
  left_join(temp_avg, by = c('location_id' = 'location_id')) %>%
  mutate(state = substr(location_id, 1, 2),
         county = substr(location_id, 4, 6)
         )
  
weather <- vrs %>%
  select(fips, 
         state
         ) %>%
  mutate(county = substr(fips, 3, 5)) %>%
  inner_join(weather_all, by = c('state' = 'state', 'county' = 'county')) %>%
  select(-state,
         -county,
         -location_id
         )

```


```{r crime adjust, eval = if (full_run) TRUE else FALSE}

# create crime data frame
crime <- crime_raw %>%
  as.data.frame()

# rename fields
names(crime) <- c('location',
                  'footnote_for_total',
                  'violent_total',
                  'footnote_for_murder_and_manslaughter',
                  'murder_and_manslaughter',
                  'footnote_for_rape',
                  'rape',
                  'footnote_for_robbery',
                  'robbery',
                  'footnote_for_assault',
                  'assault',
                  'footnote_for_2000',
                  'violent_total_2000',
                  'footnote_for_total_property_crimes',
                  'property_crimes_total',
                  'footnote_for_burglary',
                  'burglary',
                  'footnote_for_larceny_theft',
                  'larceny_theft',
                  'footnote_for_motor_vehicle_theft',
                  'motor_vehicle_theft',
                  'property_crimes_total_2000'
                  )

# separate county and state
crime_location_split <- str_split(crime$location, ', ', simplify = T) %>%
  as.data.frame() %>%
  select(county = V1,
         state = V2) %>%
  mutate(state = str_replace_all(state, ' 4', '')) %>%
  mutate(state = str_replace_all(state, ' 5', '')) %>%
  mutate(state = str_replace_all(state, ' 6', '')) %>%
  mutate(state = str_replace_all(state, ' 7', '')) 

# filter for counties level data only
crime <- crime %>%
  bind_cols(crime_location_split) %>%
  filter(nchar(state) == 2)
         
# define target variables
crime_vars <- c('violent_total',
                'murder_and_manslaughter',
                'rape',
                'robbery',
                'assault',
                'violent_total_2000',
                'property_crimes_total',
                'burglary',
                'larceny_theft',
                'motor_vehicle_theft',
                'property_crimes_total_2000'
                )

# replace character keys for null/zero and convert variables to number
crime <- crime %>%
  mutate_at(vars(crime_vars), ~ str_replace_all(., '(NA)', as.character(NA))) %>%
  mutate_at(vars(crime_vars), ~ str_replace_all(., '(X)', as.character(NA))) %>%
  mutate_at(vars(crime_vars), ~ str_replace_all(., '-', "0")) %>%
  mutate_at(vars(crime_vars), as.numeric)
  
# crime by fips
cmap <- vrs %>%
  select(fips, state, county) %>%
  mutate(county = str_replace_all(county, ' County', '')) %>%
  mutate(county = str_replace_all(county, ' City and Borough', '')) %>%
  mutate(county = str_replace_all(county, ' Borough', '')) %>%
  mutate(county = str_replace_all(county, ' Census Area', '')) %>%
  mutate(county = str_replace_all(county, ' Municipality', '')) %>%
  mutate(county = str_replace_all(county, ' Parish', '')) %>%
  mutate(county = str_replace_all(county, ' city', '')) %>%
  left_join(crime, by = c('state' = 'state', 'county' = 'county')) %>%
  group_by(fips) %>%
  summarize(violent_total = sum(violent_total),
            murder_and_manslaughter = sum(murder_and_manslaughter),
            rape = sum(rape),
            robbery = sum(robbery),
            assault = sum(assault),
            violent_total_2000 = sum(violent_total_2000),
            property_crimes_total = sum(property_crimes_total),
            burglary = sum(burglary),
            larceny_theft = sum(larceny_theft),
            motor_vehicle_theft = sum(motor_vehicle_theft),
            property_crimes_total_2000 = sum(property_crimes_total_2000)
            ) %>%
  filter(!is.na(violent_total))

# # check for unmatched
# cmap_check <- cmap %>%
#   filter(is.na(location)) %>%
#   arrange(state, county)
# 
# # check for dupes
# crime_dupes <- crime %>%
#   group_by(location) %>%
#   summarize(count = n()) %>%
#   filter(count > 1)

# combine new york city then redistribute rates
crime_ny <- cmap %>%
  filter(fips == '36047') %>% # Kings County NY
  mutate(violent_total_rate = violent_total / nyc_pop$nyc_pop,
         murder_and_manslaughter_rate = murder_and_manslaughter / nyc_pop$nyc_pop,
         rape_rate = rape / nyc_pop$nyc_pop,
         robbery_rate = robbery / nyc_pop$nyc_pop,
         assault_rate = assault / nyc_pop$nyc_pop,
         burglary_rate = burglary / nyc_pop$nyc_pop,
         larceny_theft_rate = larceny_theft / nyc_pop$nyc_pop,
         motor_vehicle_theft_rate = motor_vehicle_theft / nyc_pop$nyc_pop,
         
         all_crimes = (coalesce(violent_total, 0) + coalesce(property_crimes_total, 0)),
         murder_and_manslaughter_pct = murder_and_manslaughter / all_crimes,
         rape_pct = rape / all_crimes,
         robbery_pct = robbery / all_crimes,
         assault_pct = assault / all_crimes,
         burglary_pct = burglary / all_crimes,
         larceny_theft_pct = larceny_theft / all_crimes,
         motor_vehicle_theft_pct = motor_vehicle_theft / all_crimes
         ) %>%
  select(violent_total_rate,
         murder_and_manslaughter_rate,
         rape_rate,
         robbery_rate,
         assault_rate,
         burglary_rate,
         larceny_theft_rate,
         motor_vehicle_theft_rate,
         murder_and_manslaughter_pct,
         rape_pct,
         robbery_pct,
         assault_pct,
         burglary_pct,
         larceny_theft_pct,
         motor_vehicle_theft_pct
  ) %>%
  mutate(key = 1)

crime_ny_all <- vrs %>%
    filter(fips == '36005' |   # Bronx
           fips == '36047' | # Brooklyn Kings County
           fips == '36061' | # Manhattan
           fips == '36081' | # Queens
           fips == '36085'   # Staten Island Richmond
           ) %>%
  select(fips) %>%
  mutate(key = 1) %>%
  inner_join(crime_ny, by = c('key' = 'key')) %>%
  select(-key)


crime_all <- cmap %>%
  inner_join(tpop, by = c('fips' = 'fips')) %>%
  filter(fips != '36047') %>% # Kings County NY
  mutate(violent_total_rate = violent_total / tot_pop,
         murder_and_manslaughter_rate = murder_and_manslaughter / tot_pop,
         rape_rate = rape / tot_pop,
         robbery_rate = robbery / tot_pop,
         assault_rate = assault / tot_pop,
         burglary_rate = burglary / tot_pop,
         larceny_theft_rate = larceny_theft / tot_pop,
         motor_vehicle_theft_rate = motor_vehicle_theft / tot_pop,
         
         all_crimes = (coalesce(violent_total, 0) + coalesce(property_crimes_total, 0)),
         murder_and_manslaughter_pct = murder_and_manslaughter / all_crimes,
         rape_pct = rape / all_crimes,
         robbery_pct = robbery / all_crimes,
         assault_pct = assault / all_crimes,
         burglary_pct = burglary / all_crimes,
         larceny_theft_pct = larceny_theft / all_crimes,
         motor_vehicle_theft_pct = motor_vehicle_theft / all_crimes
         ) %>%
  select(fips,
         violent_total_rate,
         murder_and_manslaughter_rate,
         rape_rate,
         robbery_rate,
         assault_rate,
         burglary_rate,
         larceny_theft_rate,
         motor_vehicle_theft_rate,
         murder_and_manslaughter_pct,
         rape_pct,
         robbery_pct,
         assault_pct,
         burglary_pct,
         larceny_theft_pct,
         motor_vehicle_theft_pct
  ) %>%
  bind_rows(crime_ny_all)

# manual fill Benton, IL with zero rates, since they had zero reported crimes and div/0
crime_all[crime_all$fips == '18007' & is.na(crime_all)] <- 0

```

```{r environmental quality adjust, eval = if (full_run) TRUE else FALSE}

# Environmental Quality Data
eq <- eq_raw %>%
  as.data.frame()

# convert fips field to 5 characters
eq$fips <- as.character(eq$fips)
eq$fips <- ifelse(nchar(eq$fips) == 4, paste0('0', eq$fips), eq$fips)

eq <- eq %>%
  select(fips,
         
         # AIR
         a_pm25_mean, # particulate matter under 2.5 micrometers
         a_pb_ln, # lead compounds
         a_meoh_ln, # methanol
         
         # WATER
         sewagenpdesperkm, # sewage permits per 1000 km of stream in county
         indnpdesperkm, # industrial permits per 1000 km of stream in county
         stormnpdesperkm, # storwater permits per 1000 km of stream in county
         per_totpopss_ave, # % of population on self supply
         
         # LAND
         farms_per_acre_ln, # farms per acre
         pct_au_ln, # animal units per acre
         
         # BUILT
         hwyprop, # proportion of roads that are highway, miles highways / miles total roads
         ryprop, # proportion of roads that are primary streets, miles primary streets / miles total roads
         traffic_fatal_rate = fatal_rate_log, # traffic fatality rate
         pct_pub_transport_log, # % of population using public transportation
         to_unit_rate_log, # total subsidized housing units / population
         
         # business type / population
         rate_al_pn_gm_env_log, # vice
         rate_ent_env_log, # entertainment
         rate_ed_env_log, # education
         rate_food_env_neg, # negative food
         rate_food_env_pos_log, # positive food
         rate_hc_env_log, # healthcare
         rate_rec_env_log, # recreation
         rate_trans_env_log, # transportation
         rate_civic_env_log, # civic related
         
         # SOCIODEMOGRAPHIC
         pct_rent_occ, # percent renter occupied
         pct_vac_units, # pct vacant units
         med_hh_value, # median household value
         work_out_co, # pct working outside country
         med_rooms, # median number of rooms per house
         violent_rate_log # mean number of violent crimes per capita
         )

```


```{r combined dataset, eval = if (full_run) TRUE else FALSE}

# combined data
combined_df <- vrs %>%
  left_join(age_group, by = c('fips' = 'fips')) %>%
  left_join(dpop, by = c('fips' = 'fips')) %>%
  left_join(emp, by = c('fips' = 'fips')) %>%
  left_join(edu, by = c('fips' = 'fips')) %>%
  left_join(poverty, by = c('fips' = 'fips')) %>%
  left_join(et, by = c('fips' = 'fips')) %>%
  left_join(election, by = c('fips' = 'fips')) %>%
  left_join(religion_summary, by = c('fips' = 'fips')) %>%
  left_join(cases, by = c('fips' = 'fips')) %>%
  left_join(births, by = c('fips' = 'fips')) %>%
  left_join(weather, by = c('fips' = 'fips')) %>%
  left_join(crime_all, by = c('fips' = 'fips')) %>%
  left_join(eq, by = c('fips' = 'fips'))

# glimpse(df)


# clear existing objects
rm(age_group,
   births,
   births_raw,
   births2_raw,
   cases,
   cases_raw,
   cmap,
   crime,
   crime_all,
   crime_location_split,
   crime_ny,
   crime_ny_all,
   crime_raw,
   dpop,
   edu,
   edu_raw,
   # election,
   election_raw,
   emp,
   emp_raw,
   eq,
   eq_raw,
   et,
   et_raw,
   nyc_case_rates,
   nyc_cases,
   nyc_pop,
   overall_rate,
   pop,
   pop_raw,
   pov_raw,
   poverty,
   precip,
   precip_raw,
   rel_raw,
   religion,
   religion_summary,
   state_rate,
   temp_avg,
   temp_avg_raw,
   temp_max,
   temp_max_raw,
   temp_min,
   temp_min_raw,
   # tpop,
   # vr_raw,
   vrs,
   weather,
   weather_all
   )

gc()

```

```{r missing data check, eval = F}

# # plot fields with NA
# combined_df %>%
#   inspect_na %>%
#   filter(cnt > 0) %>%
#   show_plot()
# 
# # Election data does not line up for Alaska
# x <- combined_df %>%
#   filter(is.na(voter_turnout))
# 
# # 02158 and 46102 do not have economic topology data
# x <- combined_df %>%
#   filter(is.na(metro_status))
# 
# # some counties missing from weather data
# # Hawaii, DC, and Lexington VA
# x <- combined_df %>%
#   filter(is.na(precipitation))

# # NYC cases combined and not split by county
# x <- combined_df %>%
#   filter(is.na(case_rate))
# 
# xx <- cases_raw %>%
#   filter(date == snap_date) %>%
#   filter(is.na(fips))
# 
# xx <- cases_raw %>%
#   filter(date == snap_date) %>%
#   filter(state == 'New York')
# 
# xxx <- combined_df %>%
#   filter(state == 'NY')

# # 201 rows without crime data
# x <- combined_df %>%
#   filter(is.na(violent_total_rate))

# # Benton, IN crime zero, div/0 error
# x <- combined_df %>%
#   filter(!is.na(violent_total_rate) & is.na(robbery_pct))

# eq missing
# x <- combined_df %>%
#   filter(is.na(a_pm25_mean))
# 
# 
# # # check final df for nulls again
# df %>%
#   inspect_na %>%
#   filter(cnt > 0) %>%
#   show_plot()

```

```{r missing data plot, eval = if (full_run) TRUE else FALSE}
VIM::aggr(combined_df, 
          col=c('green','red'), 
          numbers = T, 
          sortVars = T,
          cex.axis = .5,
          ylab=c("Proportion of Data", "Combinations and Percentiles")
          ) 
```

```{r train test split, eval = if (full_run) TRUE else FALSE}

# dataframe without missing values
df <- combined_df %>%
  filter(!is.na(combined_df$voter_turnout) & 
           !is.na(metro_status) & 
           !is.na(precipitation) &
           !is.na(violent_total_rate) &
           !is.na(a_pm25_mean) 
         ) %>%
  select(-fips,
         -state,
         -county)

# stratified train/test split
set.seed(101)
trainIndex <- createDataPartition(df$vrate,
                                  p = 0.75,
                                  list = F)

train <- df[trainIndex,]
test <- df[-trainIndex,]

ctrl <- trainControl(method = 'cv', number = 10)

```

There are 2,888 observations or counties after excluding Texas and island territories. After all the variables have been joined, there are 233 counties with missing data which have been omitted from the final dataset or leaves 2,665 observations. This includes 1 independent variable, vaccination rate, and 125 dependent variables.

\pagebreak

# Statistical Methods

Using vaccination rates as the dependent variable, we can create supervised machine learning models to determine which variables have the highest impact. The independent variables will be county level data sources which are made available by the government and the dependent variables have been sourced from the other government and other organizations. 

With 2,665 observations, the data will be split into 75% as a training group with 2001 observations and 25% as a test group with 664 observations. Each of the machine learning models will use all the variables and we will determine the final model which has the lowest validation error. After determining the relatively best model, we can assess the variable importance levels.

Finally, we can check the distributions of vaccination rates based on the most important variables and assign classes (low / mid / high). We will create two forecasts, one using the overall US vaccination rates as a time series, and another using each of the classes as independent time series. The latter group will be combined, weighted by population, and compared to the overall forecast. This will allow us to estimate when the US might reach herd immunity of 70% vaccinated. 

The rates provided are based on the population considered, so the real rates including all counties will vary.

The final time series projection will be selected based on the error of test data using the most recent vaccination rates, held out from the data used to train the models.

```{r, eval = if (full_run) TRUE else FALSE}

# Ridge
library(lars)
ridgeGrid <- data.frame(.lambda = seq(0, .1, length = 15))

# fit model
ridgeRegFit <- train(vrate ~ . ,
                     data = train,
                     method = 'ridge',
                     tuneGrid = ridgeGrid,
                     trControl = ctrl,
                     preProc = c('center','scale'))


# Lasso
library(glmnet)
lambdas <- 10^seq(2, -3, by = -.1)
lasso <- train(vrate ~ . ,
               data = train,
               method = 'glmnet',
               tuneGrid = expand.grid(alpha = 1, lambda=lambdas)
               )


# knn
knnModel <- train(vrate ~ . ,
                  data = train,
                  method = "knn",
                  preProc = c("center", "scale"),
                  tuneLength = 10)
knnModel




# neural network
nn_grid <- expand.grid(.decay = c(0, 0.01, 0.1),
                       .size = c(1:10),
                       .bag = F)

nn_tune <- train(vrate ~ . ,
                 data = train,
                 method = 'avNNet',
                 tuneGrid = nn_grid,
                 trControl = ctrl,
                 preProc = c('center', 'scale'),
                 linout = T,
                 trace = F,
                 maxit = 500
                 )

# plot(nn_tune)


# MARS
library(earth)

marsGrid <- expand.grid(.degree = 1:2, .nprune = 2:38)

marsTuned <- train(vrate ~ . ,
                   data = train,
                   method = 'earth',
                   tuneGrid = marsGrid,
                   trControl = trainControl(method = 'cv')
                   )

# plot(marsTuned)



# Support Vector Machines
library(kernlab)

svmTuned <- train(vrate ~ . ,
                  data = train,
                  method = 'svmRadial',
                  preProc = c('center', 'scale'),
                  tuneLength = 14,
                  trControl = trainControl(method = 'cv')
                  )

# plot(svmTuned)







ranger <- train(vrate ~ . ,
                data = train,
                method = 'ranger',
                importance = 'impurity')

rf <- randomForest(vrate ~ . ,
                   data = train,
                   importance = T,
                   ntree = 1000
                   )

# boosted trees
library(gbm)

gbm <- train(vrate ~ . ,
             data = train,
             method = 'gbm', 
             verbose = F
             )

# Cubist
library(Cubist)

cubist <- train(vrate ~ . ,
             data = train,
             method = 'cubist'
             )


# bagged tree
library(ipred)

bagg <- bagging(vrate ~ ., 
                data = train
                )



```

After training the models, the predictions on the test data set are compared to determine the best predictor.

```{r model compare, eval = if (full_run) TRUE else FALSE}

ridge_predict <- predict(ridgeRegFit, test)
lasso_predict <- predict(lasso, test)

knn_predict <- predict(knnModel, test)
nn_predict <- predict(nn_tune, test)
mars_predict <- predict(marsTuned, test)
svm_predict <- predict(svmTuned, test)

ranger_predict <- predict(ranger, test)
rf_predict <- predict(rf, test)
bag_predict <- predict(bagg, test)
gbm_predict <- predict(gbm, test)
cubist_predict <- predict(cubist, test)


pred_test_results <- data.frame(rbind(
  postResample(ridge_predict, test$vrate),
  postResample(lasso_predict, test$vrate),
  
  postResample(knn_predict, test$vrate),
  postResample(nn_predict, test$vrate),
  postResample(mars_predict, test$vrate),
  postResample(svm_predict, test$vrate),
  
  postResample(ranger_predict, test$vrate),
  postResample(rf_predict, test$vrate),
  postResample(bag_predict, test$vrate),
  postResample(gbm_predict, test$vrate),
  postResample(cubist_predict, test$vrate)
),
  row.names = c('Ridge', 'LASSO',
                'KNN', 'Neural Network', 'MARS', 'SVM',
                'ranger', 'Random Forest', 'Bagging', 'Boosting', 'Cubist'
                )
)
```

\pagebreak

#### Model Validation Results

```{r modeling results from local path, eval = if (!full_run) TRUE else FALSE}

modeling_results <- read.csv(test_results_path)
important_vars <- read.csv(varimp_path)

modeling_results %>%
  arrange(RMSE)
```

#### Variable Importance

Here's a look at the most important variables of the cubist model:

```{r cubist var imp, eval = if (full_run) TRUE else FALSE}
top10_varimp <- varImp(cubist)$importance %>%
  as.data.frame() %>%
  arrange(desc(Overall)) %>%
  head(10)


write.csv(pred_test_results, test_results_path, row.names = T)
write.csv(top10_varimp, varimp_path, row.names = T)

```

```{r var imp from local path, eval = if (!full_run) TRUE else FALSE}

important_vars %>%
  arrange(desc(Overall))

```


```{r residual plot, eval = F}

test$pred <- cubist_predict
test$resid <- with(test, pred - vrate)

ggplot(test, aes(x = vrate, y = resid)) +
  geom_point(alpha = 0.3) +
  geom_hline(yintercept = 0, lty = 1, color = 'purple')


ggplot(test, aes(x = vrate, y = pred)) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, lty = 1, color = 'purple')

ggplot(df, aes(x = per_gop, y = vrate)) +
  geom_point(alpha = 0.3)
```

```{r gop distributions, eval = F}
ggplot(df, aes(x = per_gop)) +
  geom_density()

ggplot(df, aes(x = per_gop)) +
  geom_boxplot()

```

```{r data prep for time series}

# Vaccination Rate Nationwide time series
overall_vr <- vr2 %>%
  group_by(date) %>%
  summarize(tv = sum(vaccinated, na.rm = T),
            tp = sum(total_population, na.rm = T),
            pv = tv / tp
            ) %>%
  arrange(date)


# political groups time series
gop <- election %>%
  select(fips, per_gop)
  
pol_vr <- 
  vr2 %>%
  left_join(gop, by = c('fips' = 'fips')) %>%
  filter(!is.na(per_gop)) %>%
  mutate(grp = ifelse(per_gop < 0.4, 'left', ifelse(per_gop > 0.6, 'right', 'center'))) %>%
  group_by(date, grp) %>%
  summarize(tv = sum(vaccinated, na.rm = T),
            tp = sum(total_population, na.rm = T),
            pv = tv / tp,
            fips_count = n(),
            .groups = 'drop'
            ) %>%
  arrange(date)
    
left_vr <- pol_vr %>% 
  filter(grp == 'left')

center_vr <- pol_vr %>%
  filter(grp == 'center')

right_vr <- pol_vr %>%
  filter(grp == 'right')
```

#### Political Classes

```{r political group summary}

# summary of political affliation groups
pol_summary <- pol_vr[1:3,] %>%
  mutate(total_population_in_millions = round(tp / 1e6, 1),
         avg_population_per_fip_in_thousands = round(tp / fips_count / 1e3, 1)
         ) %>%
  select(grp,
         pop_in_M = total_population_in_millions,
         fips_count,
         avg_pop_per_fip_K = avg_population_per_fip_in_thousands)

pol_summary %>%
  as.data.frame()

# get population by group
left_summary <- pol_vr[1:3,] %>%
  filter(grp == 'left')

left_pop <- left_summary$tp

center_summary <- pol_vr[1:3,] %>%
  filter(grp == 'center')

center_pop <- center_summary$tp

right_summary <- pol_vr[1:3,] %>%
  filter(grp == 'right')

right_pop <- right_summary$tp

```

Since `per_gop` was by far the most important variable, we'll use it to classify each of the counties. The variable `per_gop` represents the percent of the county who voted for the GOP in the 2020 election. We can then classify each county as left leaning with less than 40% voting Republican, right leaning with greater than 60% of the county voting Republican, or in the center with 40 - 60% of the county voting Republican. 

```{r train time series models}

# number of dates
n <- nrow(overall_vr)

# train test split at 80%
train_rows <- floor(n * 0.8)
test_rows <- n - train_rows

# split training and test sets
ovr_train <- overall_vr[1:train_rows,]
ovr_test <- overall_vr[(train_rows + 1):n,]

left_train <- left_vr[1:train_rows,]
left_test <- left_vr[(train_rows + 1):n,]

center_train <- center_vr[1:train_rows,]
center_test <- center_vr[(train_rows + 1):n,]

right_train <- right_vr[1:train_rows,]
right_test <- right_vr[(train_rows + 1):n,]


# time series objects
ots <- ts(overall_vr$pv, start = c(2020,347), frequency = 365)
ots_train <- ts(ovr_train$pv, start = c(2020,347), frequency = 365)

left <- ts(left_vr$pv, start = c(2020,347), frequency = 365)
left_train <- ts(left_train$pv, start = c(2020,347), frequency = 365)

center <- ts(center_vr$pv, start = c(2020,347), frequency = 365)
center_train <- ts(center_train$pv, start = c(2020,347), frequency = 365)

right <- ts(right_vr$pv, start = c(2020,347), frequency = 365)
right_train <- ts(right_train$pv, start = c(2020,347), frequency = 365)


# create models based off training data
ots_arima <- ots_train %>% auto.arima()
ots_ets <- ots_train %>% ets()

left_arima <- left_train %>% auto.arima()
left_ets <- left_train %>% ets()

center_arima <- center_train %>% auto.arima()
center_ets <- center_train %>% ets()

right_arima <- right_train %>% auto.arima()
right_ets <- right_train %>% ets()

```

The vaccine rate data at the time of publishing has 301 days of data and we'll be using 80% or 240 days of data to train the forecasting models and the remaining 20% or 61 days of data to validate the relative predictive errors.

\pagebreak

#### Overall Forecast Results

Overall vaccine rates `auto.arima()` accuracy:

```{r eval overall ts arima}
# evaluate accuracy of arima
ots_arima %>%
  forecast(h = test_rows) %>%
  accuracy(ots)
```

Overall vaccine rates `ets()` accuracy:

```{r eval overall ts ets}
# evaluate accuracy of ets
ots_ets %>%
  forecast(h = test_rows) %>%
  accuracy(ots)
```

```{r compare overall ts models}
fa_p <- ots_arima %>%
  forecast(h = test_rows, level = 0)

fe_p <- ots_ets %>%
  forecast(h = test_rows, level = 0)

# I can't figure out how to add a legend to this!!!
ots %>%
  autoplot(color = 'black') +
  autolayer(fa_p, series = 'ARIMA') +
  autolayer(fe_p, series = 'Exponential Smoothing') +
  labs(color = 'fits',
       x = '',
       title = 'Overall US Vaccine Rates Forecast') +
  scale_x_yearmon() +
  scale_y_continuous(labels = percent) +
  theme_bw() +
  guides(color = guide_legend()) +
  theme(legend.position = 'bottom')
  
```

Looking at forecasting overall US vaccine rates using ARIMA and exponential smoothing, the ARIMA model produces a slightly more accurate forecast, which we'll use for the overall forecast.

#### Retrained Final Forecast, Overall US

```{r final overall forecast}

final_arima <- ots %>% auto.arima()

final_forecast <- final_arima %>%
  forecast(h = 365)

final_forecast2 <- final_arima %>%
  forecast(h = 365, level = 0)

ots %>%
  autoplot(color = 'black') +
  autolayer(final_forecast, color = 'purple', alpha = 0.3) +
  labs(color = 'fits',
       x = '',
       title = 'Overall Vaccine Rates Forecast',) +
  scale_x_yearmon() +
  scale_y_continuous(labels = percent_format(accuracy = 1), limits = c(0,1), breaks = seq(0,1,0.1)) +
  theme_bw() +
  geom_hline(yintercept = 0.7, lty = 3)

```

After retraining the model using the full time series, we can predict that herd immunity would be reached after 2Q 2022. The 80% prediction interval suggests a possibility of reaching 70% vaccination rates before the end of 2021 and the 95% prediction interval indicates its possible at the end of the year.

Note that the intervals on the chart do not reflect the reality that vaccination rates will not drop and can only go up.

\pagebreak

#### Political Class Forecast Results

##### Left Leaning

Left wing vaccine rates `auto.arima()` accuracy:

```{r eval left ts arima}
# evaluate accuracy of arima
left_arima %>%
  forecast(h = test_rows) %>%
  accuracy(left)
```

Left wing vaccine rates `ets()` accuracy:

```{r eval left ts ets}
# evaluate accuracy of ets
left_ets %>%
  forecast(h = test_rows) %>%
  accuracy(left)
```

```{r left forecast comparison}
left_fa_p <- left_arima %>%
  forecast(h = test_rows, level = 0)

left_fe_p <- left_ets %>%
  forecast(h = test_rows, level = 0)

left %>%
  autoplot(color = 'black') +
  autolayer(left_fa_p, series = 'ARIMA') +
  autolayer(left_fe_p, series = 'Exponential Smoothing') +
  labs(color = 'fits',
       x = '',
       title = 'Left Wing Vaccine Rates',
       subtitle = 'ARMIA in Red and ETS in blue') +
  scale_x_yearmon() +
  scale_y_continuous(labels = percent) +
  theme_bw() +
  guides(color = guide_legend()) +
  theme(legend.position = 'bottom')
  
```

For the counties who leaned left politically, the exponential smoothing model produced a more accurate forecast.

\pagebreak

##### Moderate Leaning

Center vaccine rates `auto.arima()` accuracy:

```{r eval center ts arima}
# evaluate accuracy of arima
center_arima %>%
  forecast(h = test_rows) %>%
  accuracy(center)
```

Center vaccine rates `ets()` accuracy:

```{r eval center ts ets}
# evaluate accuracy of ets
center_ets %>%
  forecast(h = test_rows) %>%
  accuracy(center)
```

```{r center forecast comparison}
center_fa_p <- center_arima %>%
  forecast(h = test_rows, level = 0)

center_fe_p <- center_ets %>%
  forecast(h = test_rows, level = 0)

center %>%
  autoplot(color = 'black') +
  autolayer(center_fa_p, series = 'ARIMA') +
  autolayer(center_fe_p, series = 'Exponential Smoothing') +
  labs(color = 'fits',
       x = '',
       title = 'Center Vaccine Rates',
       subtitle = 'ARMIA in Red and ETS in blue') +
  scale_x_yearmon() +
  scale_y_continuous(labels = percent) +
  theme_bw() +
  guides(color = guide_legend()) +
  theme(legend.position = 'bottom')
  
```

For politically moderate counties, both forecasts had low error, with ARIMA having a slight edge.

\pagebreak

##### Right Leaning

Right wing vaccine rates `auto.arima()` accuracy:

```{r eval right ts arima}
# evaluate accuracy of arima
right_arima %>%
  forecast(h = test_rows) %>%
  accuracy(right)
```

Right wing vaccine rates `ets()` accuracy:

```{r eval right ts ets}
# evaluate accuracy of ets
right_ets %>%
  forecast(h = test_rows) %>%
  accuracy(right)
```

```{r right forecast comparison}
right_fa_p <- right_arima %>%
  forecast(h = test_rows, level = 0)

right_fe_p <- right_ets %>%
  forecast(h = test_rows, level = 0)

right %>%
  autoplot(color = 'black') +
  autolayer(right_fa_p, series = 'ARIMA') +
  autolayer(right_fe_p, series = 'Exponential Smoothing') +
  labs(color = '',
       x = '',
       title = 'Right Wing Vaccine Rates',
       subtitle = 'ARMIA in Red and ETS in blue') +
  scale_x_yearmon() +
  scale_y_continuous(labels = percent_format(accuracy=1)) +
  theme_bw() +
  guides(color = guide_legend()) +
  theme(legend.position = 'bottom')
  
  
```

Although exponential smoothing produces a lower test error using RMSE, both forecast methods are underestimating, possibly due to changes in the landscape related to vaccination mandates, surges of cases, deaths, as well as the weakening influence of Donald Trump.

\pagebreak

#### Forecasts by Political Affliation Class

```{r political group forecast plot}

left_final_fc <- left %>% ets()
center_final_fc <- center %>% auto.arima()
right_final_fc <- right %>% ets()

left_fc <- left_final_fc %>%
  forecast(h = 365, level = 0)

center_fc <- center_final_fc %>%
  forecast(h = 365, level = 0)

right_fc <- right_final_fc %>%
  forecast(h = 365)

left %>%
  autoplot(color = 'blue') +
  autolayer(left_fc, series = 'Left Wing', lty = 2) +
  autolayer(right, color = 'red') +
  autolayer(right_fc, series = 'Right Wing', lty = 2, alpha = 0.2) +
  autolayer(center, color = 'orange') +
  autolayer(center_fc, series = 'Center', lty = 2) +
  labs(color = '',
       x = '',
       title = 'Vaccine Rates by Political Affliation Forecast') +
  scale_x_yearmon() +
  scale_y_continuous(labels = percent_format(accuracy = 1), breaks = seq(0,0.9,0.1)) +
  theme_bw() +
  geom_hline(yintercept = 0.7, lty = 3) +
  guides(color = guide_legend()) +
  scale_color_manual(breaks = c('Left Wing', 'Center', 'Right Wing'),
                     values = c('blue','orange', 'red')) +
  theme(legend.position = 'bottom')

```

The chart above shows each of the classes with models retrained using the methods with the lowest test error. We can see that left leaning counties have the highest rate of vaccinations, followed by moderates, with conservatives trailing. Both left and moderate leaning counties are projected to hit 70% vaccination rates before 2Q 2022, which suggests that the right wing vaccination rates are pulling down the overall average rates.

\pagebreak

#### Comparing Final Forecasts

```{r}

wtg_avg_train <- (
  (left_fe_p$mean * left_pop) + (center_fa_p$mean * center_pop) + (right_fe_p$mean * right_pop)
) / (left_pop + center_pop + right_pop)


ots %>%
  autoplot(color = 'black') +
  autolayer(fa_p, series = 'ARIMA') +
  autolayer(wtg_avg_train, series = 'Weighted by Political Affliation') +
  labs(color = '',
       x = '',
       title = 'Vaccine Rates Forecast Comparison') +
  scale_x_yearmon() +
  scale_y_continuous(labels = percent_format(accuracy = 1), breaks = seq(0,0.9,0.1)) +
  theme_bw() +
  guides(color = guide_legend()) +
  scale_color_manual(values = c('purple','brown')) +
  theme(legend.position = 'bottom')


```

Comparing the forecasts on the test set show both models with very low test error.

Overall US vaccine rates ARIMA error:

```{r eval overall ts arima again}
# evaluate accuracy of arima
ots_arima %>%
  forecast(h = test_rows) %>%
  accuracy(ots)
```

Weighted Average Time Series Test error:

```{r eval wtg avg train ts ets}
# evaluate accuracy of ets
wtg_avg_train %>%
  accuracy(ots)
```

The overall forecast seems to have a slightly lower error based on RMSE. Its possible that the change in trend seen in the right leaning class is being averaged in total and leading to a smoother trend overall.

\pagebreak

#### Final Forecast

```{r wtg avg forecast}

wtg_avg_ts <- (
  (left_fc$mean * left_pop) + (center_fc$mean * center_pop) + (right_fc$mean * right_pop)
) / (left_pop + center_pop + right_pop)

ots %>%
  autoplot(color = 'black') +
  autolayer(final_forecast2, series = 'Overall') +
  autolayer(wtg_avg_ts, series = 'Weighted by Political Affliation') +
  labs(color = '',
       x = '',
       title = 'Vaccine Rates Forecast Comparison') +
  scale_x_yearmon() +
  scale_y_continuous(labels = percent_format(accuracy = 1), breaks = seq(0,0.9,0.1)) +
  theme_bw() +
  geom_hline(yintercept = 0.7, lty = 3) +
  guides(color = guide_legend()) +
  scale_color_manual(values = c('purple','brown')) +
  theme(legend.position = 'bottom')
  

```

Above, we see the comparison of the overall US forecast compared to the combined forecasts of political classes, weighted by population. Both models above have been retrained using the entire time series. 

The politically weighted forecast slows down the overall rate and suggests herd immunity can be reached between 2Q and 3Q 2022, about a month later than the overall US model.

\pagebreak

### Discussion of Results

The attempt to gather as many variables as possible did not lead to any surprising results and resulted in validating earlier research relating to the online behavior of conservatives [@social_media]. The percent of the population who graduated from college was the 2nd most important variable, but still about half as important as the percent of the voters in a county who voted for Donald Trump. The percent of people who did not graduate high school was the 4th most important variable, further validating the assumption that education affects an individual's ability to understand vaccination and their susceptibility to misinformation online. 

The average age of mother at birth and the average weight of a baby were the 3rd and 5th most important variables in the leading cubist model, which may suggest that the lifestyle and health of mothers in a given county could predict people's affinity to vaccines. Again, both of these factors were about half as important as the percent of voters who voted for Donald Trump. It's likely that these factors are not directly related to vaccination rates and instead are related to or collinear to other factors not included in this research.

Finally, using the data we have available, we can project that the US could hit herd immunity before the start of summer 2022. This would be possible if we continued vaccinating the population at the current pace, but it's entirely possible the rate of vaccinations slows and reaches a plateau as we face the sector of the population who refuse to vaccinate. This could keep us from reaching herd immunity as a nation, even if certain left leaning and moderate counties may have already reached over 70% vaccinated.

Validating earlier research suggests that improving education could have long term benefits for receptiveness to the understanding of vaccines as well as reducing the impact of misinformation, while the short term strategy includes fighting misinformation and educate the public about vaccines.

\pagebreak

# References\